---
title: モデル
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';
import modelCustomProviderExample from '../../../../../../examples/docs/models/customProviders.ts?raw';
import setDefaultOpenAIKeyExample from '../../../../../../examples/docs/config/setDefaultOpenAIKey.ts?raw';
import modelSettingsExample from '../../../../../../examples/docs/models/modelSettings.ts?raw';
import promptIdExample from '../../../../../../examples/basic/prompt-id.ts?raw';
import agentWithModelExample from '../../../../../../examples/docs/models/agentWithModel.ts?raw';
import runnerWithModelExample from '../../../../../../examples/docs/models/runnerWithModel.ts?raw';
import gpt5DefaultModelSettingsExample from '../../../../../../examples/docs/models/gpt5DefaultModelSettings.ts?raw';
import setDefaultModelProviderExample from '../../../../../../examples/docs/models/setDefaultModelProvider.ts?raw';
import setTracingExportApiKeyExample from '../../../../../../examples/docs/config/setTracingExportApiKey.ts?raw';

すべての Agent は最終的に LLM を呼び出します。SDK は、モデルを次の 2 つの軽量インターフェースの背後に抽象化します:

- [`Model`](/openai-agents-js/openai/agents/interfaces/model) – 特定の API に対して _1 回_ のリクエストを実行する方法を認識します
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider) – 人間が読みやすいモデル **名**（例: `'gpt‑5.2'`）を `Model` インスタンスに解決します

日常的な利用では、通常はモデル **名** を扱い、必要に応じて `ModelSettings` を扱います。

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="エージェントごとのモデル指定"
/>

## モデルの選択

### デフォルトモデル

`Agent` の初期化時にモデルを指定しない場合、デフォルトモデルが使われます。現在のデフォルトは、互換性と低レイテンシのために [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) です。利用可能であれば、明示的な `modelSettings` を維持しつつ、より高品質な [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) をエージェントに設定することを推奨します。

[`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) など他のモデルに切り替えるには、エージェントを設定する方法が 2 つあります。

1 つ目は、カスタムモデルを設定していないすべてのエージェントで特定のモデルを一貫して使用したい場合です。エージェント実行前に `OPENAI_DEFAULT_MODEL` 環境変数を設定します。

```bash
export OPENAI_DEFAULT_MODEL=gpt-5.2
node my-awesome-agent.js
```

2 つ目は、`Runner` インスタンスにデフォルトモデルを設定する方法です。エージェントにモデルを設定しない場合、この `Runner` のデフォルトモデルが使われます。

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="Runner のデフォルトモデル設定"
/>

#### GPT-5.x モデル

この方法で [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) などの GPT-5.x モデルを使うと、SDK はデフォルトの `modelSettings` を適用します。多くのユースケースで最適に動作する設定が使われます。デフォルトモデルの推論 effort を調整するには、独自の `modelSettings` を渡します:

<Code
  lang="typescript"
  code={gpt5DefaultModelSettingsExample}
  title="GPT-5 のデフォルト設定のカスタマイズ"
/>

より低レイテンシにするには、`gpt-5.2` で `reasoning.effort: "none"` を使うことを推奨します。`gpt-4.1` ファミリー（ mini / nano を含む）も、対話型エージェントアプリ構築で引き続き有力な選択肢です。

#### 非 GPT-5 モデル

カスタム `modelSettings` なしで非 GPT-5 のモデル名を渡すと、SDK は任意のモデルと互換性のある汎用 `modelSettings` に戻します。

---

## OpenAI プロバイダー設定

### OpenAI プロバイダー

デフォルトの `ModelProvider` は OpenAI API を使って名前を解決します。以下の 2 つのエンドポイントをサポートします:

| API              | 用途                                                                     | `setOpenAIAPI()` の呼び出し             |
| ---------------- | ------------------------------------------------------------------------ | --------------------------------------- |
| Chat Completions | 標準のチャットと関数呼び出し                                             | `setOpenAIAPI('chat_completions')`      |
| Responses        | 新しい ストリーミング ファーストの生成 API（ツール呼び出し、柔軟な出力） | `setOpenAIAPI('responses')` _(default)_ |

#### 認証

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="OpenAI のデフォルトキー設定"
/>

カスタムネットワーク設定が必要な場合は、`setDefaultOpenAIClient(client)` で独自の `OpenAI` クライアントを接続することもできます。

#### Responses WebSocket トランスポート

Responses API で OpenAI プロバイダーを使う場合、デフォルトの HTTP トランスポートではなく WebSocket トランスポートでリクエストを送信できます。

`setOpenAIResponsesTransport('websocket')` でグローバルに有効化するか、`new OpenAIProvider({ useResponses: true, useResponsesWebSocket: true })` でプロバイダー単位に有効化します。

WebSocket トランスポートを使うだけなら、`withResponsesWebSocketSession(...)` やカスタム `OpenAIProvider` は不要です。各 run / request ごとの再接続を許容できる場合、`setOpenAIResponsesTransport('websocket')` を有効化した後も既存の `run()` / `Runner.run()` はそのまま動作します。

トランスポートの選択はモデル解決に従います:

- `setOpenAIResponsesTransport('websocket')` は、Responses API を使う際に OpenAI プロバイダー経由で後から解決される文字列モデル名にのみ影響します
- `Agent` や `Runner` に具体的な `Model` インスタンスを渡した場合、そのインスタンスがそのまま使われます。`OpenAIResponsesWSModel` は WebSocket のまま、`OpenAIResponsesModel` は HTTP のまま、`OpenAIChatCompletionsModel` は Chat Completions のままです
- 独自の `modelProvider` を指定した場合、モデル解決はそのプロバイダーが制御します。グローバル setter ではなく、そちらで WebSocket を有効化してください
- プロキシ、ゲートウェイ、またはその他の OpenAI 互換エンドポイントを経由する場合、接続先が WebSocket `/responses` エンドポイントをサポートしている必要があります。`websocketBaseURL` を明示的に設定する必要がある場合もあります

接続再利用の最適化や websocket プロバイダーのライフサイクルをより明示的に管理したい場合のみ、`withResponsesWebSocketSession(...)` またはカスタム `OpenAIProvider` / `Runner` を使ってください:

- `withResponsesWebSocketSession(...)`: コールバック後に自動クリーンアップされる便利なスコープ付きライフサイクル
- カスタム `OpenAIProvider` / `Runner`: 独自アプリ構成での明示的なライフサイクル制御（終了時クリーンアップを含む）

名前に反して、`withResponsesWebSocketSession(...)` はトランスポートのライフサイクルヘルパーであり、[セッション ガイド](/openai-agents-js/ja/guides/sessions) で説明されるメモリー `Session` インターフェースとは無関係です。

websocket プロキシやゲートウェイを使う場合は、`OpenAIProvider` の `websocketBaseURL` を設定するか、`OPENAI_WEBSOCKET_BASE_URL` を設定してください。

`OpenAIProvider` を自分でインスタンス化する場合、websocket ベースの Responses モデルラッパーは接続再利用のためにデフォルトでキャッシュされる点に注意してください。終了時に `await provider.close()` を呼び出して、これらのキャッシュ接続を解放します。`withResponsesWebSocketSession(...)` は主にこのライフサイクル管理のために存在します。websocket 有効な provider と runner を作成し、コールバックに渡し、その後必ず provider を閉じます。一時 provider には `providerOptions`、コールバックスコープの runner 既定値には `runnerConfig` を使います。

Responses WebSocket トランスポートを使った完全な ストリーミング + HITL 例は [`examples/basic/stream-ws.ts`](https://github.com/openai/openai-agents-js/tree/main/examples/basic/stream-ws.ts) を参照してください。

---

## モデルの挙動とプロンプト

### ModelSettings

`ModelSettings` は OpenAI のパラメーターを反映しつつ、プロバイダー非依存です。

| Field                  | Type                                                            | Notes                                                                          |
| ---------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| `temperature`          | `number`                                                        | 創造性と決定性のバランス                                                       |
| `topP`                 | `number`                                                        | Nucleus sampling                                                               |
| `frequencyPenalty`     | `number`                                                        | 繰り返しトークンへのペナルティ                                                 |
| `presencePenalty`      | `number`                                                        | 新しいトークンの出現を促進                                                     |
| `toolChoice`           | `'auto' \| 'required' \| 'none' \| string`                      | [ツール使用の強制](/openai-agents-js/ja/guides/agents#forcing-tool-use) を参照 |
| `parallelToolCalls`    | `boolean`                                                       | サポートされる場合に並列関数呼び出しを許可                                     |
| `truncation`           | `'auto' \| 'disabled'`                                          | トークン切り詰め戦略                                                           |
| `maxTokens`            | `number`                                                        | レスポンス内の最大トークン数                                                   |
| `store`                | `boolean`                                                       | 取得 / RAG ワークフロー用にレスポンスを永続化                                  |
| `promptCacheRetention` | `'in-memory' \| '24h' \| null`                                  | サポート時のプロバイダー prompt cache 保持期間を制御                           |
| `reasoning.effort`     | `'none' \| 'minimal' \| 'low' \| 'medium' \| 'high' \| 'xhigh'` | gpt-5.x モデルの推論 effort                                                    |
| `reasoning.summary`    | `'auto' \| 'concise' \| 'detailed'`                             | モデルが返す推論要約の量を制御                                                 |
| `text.verbosity`       | `'low' \| 'medium' \| 'high'`                                   | gpt-5.x などでのテキスト冗長度                                                 |
| `providerData`         | `Record<string, any>`                                           | 下位モデルへ転送されるプロバイダー固有のパススルーオプション                   |

設定は次のいずれのレベルにも適用できます:

<Code lang="typescript" code={modelSettingsExample} title="モデル設定" />

`Runner` レベルの設定は、競合するエージェント単位の設定を上書きします。

---

### プロンプト

エージェントは `prompt` パラメーターで設定できます。これは、Agent の挙動を制御するために使うサーバー保存済みプロンプト設定を示します。現在、このオプションは OpenAI の [Responses API](https://platform.openai.com/docs/api-reference/responses) を使う場合のみサポートされます。

| Field       | Type     | Notes                                                                                                                         |
| ----------- | -------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `promptId`  | `string` | プロンプトの一意な識別子                                                                                                      |
| `version`   | `string` | 使用したいプロンプトのバージョン                                                                                              |
| `variables` | `object` | プロンプトに代入する変数のキー / 値ペア。値には文字列のほか、テキスト・画像・ファイルなどの content input type を使用できます |

<Code
  lang="typescript"
  code={promptIdExample}
  title="プロンプト付きエージェント"
/>

ツールや instructions などの追加エージェント設定は、保存済みプロンプトで設定されている値を上書きします。

---

## 高度なプロバイダーと可観測性

### カスタムモデルプロバイダー

独自プロバイダーの実装は簡単です。`ModelProvider` と `Model` を実装し、`Runner` コンストラクターにプロバイダーを渡します:

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="最小構成のカスタムプロバイダー"
/>

すべての `run()` 呼び出しと新規作成される `Runner` で同じプロバイダーをデフォルト利用したい場合は、アプリ起動時に 1 回設定します:

<Code
  lang="typescript"
  code={setDefaultModelProviderExample}
  title="デフォルトモデルプロバイダーの設定"
/>

これは、アプリで非 OpenAI プロバイダーを標準化し、どこでもカスタム `Runner` を渡したくない場合に便利です。

非 OpenAI モデル向けの既成アダプターが必要な場合は、[AI SDK で任意モデルを指定](/openai-agents-js/ja/extensions/ai-sdk) を参照してください。

---

### トレーシング認証情報

サポートされるサーバーランタイムでは、トレーシングはすでにデフォルトで有効です。トレースエクスポートでデフォルトの OpenAI API キーとは異なる認証情報を使う必要がある場合のみ、`setTracingExportApiKey()` を使用します。

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="トレーシング エクスポート API キーの設定"
/>

これにより、その認証情報を使って [OpenAI dashboard](https://platform.openai.com/traces) にトレースが送信されます。カスタム ingest endpoint や retry 調整など exporter のカスタマイズについては、[トレーシング ガイド](/openai-agents-js/ja/guides/tracing#openai-tracing-exporter) を参照してください。

---

## 次のステップ

- [エージェントの実行](/openai-agents-js/ja/guides/running-agents) を確認する
- [ツール](/openai-agents-js/ja/guides/tools) でモデルをさらに強化する
- 必要に応じて [ガードレール](/openai-agents-js/ja/guides/guardrails) や [トレーシング](/openai-agents-js/ja/guides/tracing) を追加する
