---
title: クイックスタート
description: Build your first realtime voice assistant using the OpenAI Agents SDK in minutes.
---

import { Steps, Aside, Code } from '@astrojs/starlight/components';
import helloWorldExample from '../../../../../../../examples/docs/voice-agents/helloWorld.ts?raw';
import createAgentExample from '../../../../../../../examples/docs/voice-agents/createAgent.ts?raw';
import multiAgentsExample from '../../../../../../../examples/docs/voice-agents/multiAgents.ts?raw';
import createSessionExample from '../../../../../../../examples/docs/voice-agents/createSession.ts?raw';
import configureSessionExample from '../../../../../../../examples/docs/voice-agents/configureSession.ts?raw';
import handleAudioExample from '../../../../../../../examples/docs/voice-agents/handleAudio.ts?raw';
import defineToolExample from '../../../../../../../examples/docs/voice-agents/defineTool.ts?raw';
import toolApprovalEventExample from '../../../../../../../examples/docs/voice-agents/toolApprovalEvent.ts?raw';
import guardrailsExample from '../../../../../../../examples/docs/voice-agents/guardrails.ts?raw';
import guardrailSettingsExample from '../../../../../../../examples/docs/voice-agents/guardrailSettings.ts?raw';
import audioInterruptedExample from '../../../../../../../examples/docs/voice-agents/audioInterrupted.ts?raw';
import sessionInterruptExample from '../../../../../../../examples/docs/voice-agents/sessionInterrupt.ts?raw';
import sessionHistoryExample from '../../../../../../../examples/docs/voice-agents/sessionHistory.ts?raw';
import historyUpdatedExample from '../../../../../../../examples/docs/voice-agents/historyUpdated.ts?raw';
import updateHistoryExample from '../../../../../../../examples/docs/voice-agents/updateHistory.ts?raw';
import customWebRTCTransportExample from '../../../../../../../examples/docs/voice-agents/customWebRTCTransport.ts?raw';
import websocketSessionExample from '../../../../../../../examples/docs/voice-agents/websocketSession.ts?raw';
import transportEventsExample from '../../../../../../../examples/docs/voice-agents/transportEvents.ts?raw';
import thinClientExample from '../../../../../../../examples/docs/voice-agents/thinClient.ts?raw';

## プロジェクトセットアップと認証情報

<Steps>

1. **プロジェクトの作成**

   このクイックスタートでは、ブラウザで使える音声エージェントを作成します。新規プロジェクトを試したい場合は、[`Next.js`](https://nextjs.org/docs/getting-started/installation) や [`Vite`](https://vite.dev/guide/installation.html) を使うことができます。

   ```bash
   npm create vite@latest my-project -- --template vanilla-ts
   ```

2. **Agents SDK のインストール**（Zod v4 が必要）

   ```bash
   npm install @openai/agents zod
   ```

   代わりに、スタンドアロンのブラウザ用パッケージである `@openai/agents-realtime` をインストールすることもできます。

3. **クライアントのエフェメラルトークンを生成**

   このアプリケーションはユーザーのブラウザで実行されるため、Realtime API を介してモデルに安全に接続する必要があります。そのために、バックエンドのサーバーで生成すべき [ephemeral client key](https://platform.openai.com/docs/guides/realtime#creating-an-ephemeral-token) を使用できます。テスト目的では、`curl` と通常の OpenAI API キーを使ってキーを生成することもできます。

   ```bash
   export OPENAI_API_KEY="sk-proj-...(your own key here)"
   curl -X POST https://api.openai.com/v1/realtime/client_secrets \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "session": {
          "type": "realtime",
          "model": "gpt-realtime"
        }
      }'
   ```

   レスポンスにはトップレベルに "value" という文字列が含まれ、"ek\_" プレフィックスで始まります。このエフェメラルキーを使って後で WebRTC 接続を確立できます。このキーは短時間のみ有効で、再生成が必要になる点に注意してください。

</Steps>

## 音声エージェントの作成と接続

<Steps>

1. **最初のエージェントの作成**

   新しい [`RealtimeAgent`](/openai-agents-js/openai/agents-realtime/classes/realtimeagent/) の作成は、通常の [`エージェント`](/openai-agents-js/ja/guides/agents) の作成と非常によく似ています。

   ```typescript
   import { RealtimeAgent } from '@openai/agents/realtime';

   const agent = new RealtimeAgent({
     name: 'Assistant',
     instructions: 'You are a helpful assistant.',
   });
   ```

2. **セッションの作成**

   通常のエージェントと異なり、Voice Agent は会話と時間をかけたモデルへの接続を処理する `RealtimeSession` の中で継続的に実行・待機します。このセッションは音声処理、割り込み、その他多くのライフサイクル機能も処理します。これらは後ほど説明します。

   ```typescript
   import { RealtimeSession } from '@openai/agents/realtime';

   const session = new RealtimeSession(agent, {
     model: 'gpt-realtime',
   });
   ```

   `RealtimeSession` のコンストラクターは最初の引数として `agent` を受け取ります。このエージェントが、ユーザーが最初に対話できるエージェントになります。

3. **セッションへの接続**

   セッションに接続するには、先ほど生成したクライアントのエフェメラルトークンを渡す必要があります。

   ```typescript
   await session.connect({ apiKey: 'ek_...(put your own key here)' });
   ```

   これにより、ブラウザで WebRTC を使用して Realtime API へ接続し、マイクとスピーカーを自動的に音声入出力用に設定します。`RealtimeSession` をバックエンドのサーバー（たとえば Node.js）で実行している場合、SDK は自動的に WebSocket を接続として使用します。異なるトランスポート層の詳細は、[リアルタイムトランスポート](/openai-agents-js/ja/guides/voice-agents/transport) ガイドで確認できます。

</Steps>

## アプリの実行とテスト

<Steps>

1. **すべてを組み合わせる**

   <Code lang="typescript" code={helloWorldExample} />

2. **起動して話しかける**

   Web サーバーを起動し、新しい Realtime Agent のコードを含むページにアクセスします。マイクへのアクセス許可を求めるリクエストが表示されるはずです。許可すると、エージェントに話しかけられるようになります。

   ```bash
   npm run dev
   ```

</Steps>

## 次のステップ

ここから、独自の音声エージェントを設計・構築できます。音声エージェントには通常のエージェントと同様の機能が多く含まれますが、独自の機能もあります。

- 音声エージェントに以下を追加する方法
  - [ツール](/openai-agents-js/ja/guides/voice-agents/build#tools)
  - [ハンドオフ](/openai-agents-js/ja/guides/voice-agents/build#handoffs)
  - [ガードレール](/openai-agents-js/ja/guides/voice-agents/build#guardrails)
  - [音声の割り込みの処理](/openai-agents-js/ja/guides/voice-agents/build#interruptions)
  - [セッション履歴の管理](/openai-agents-js/ja/guides/voice-agents/build#conversation-history-management)

- さまざまなトランスポート層の詳細
  - [WebRTC](/openai-agents-js/ja/guides/voice-agents/transport#connecting-over-webrtc)
  - [WebSocket](/openai-agents-js/ja/guides/voice-agents/transport#connecting-over-websocket)
  - [独自のトランスポート機構の構築](/openai-agents-js/ja/guides/voice-agents/transport#building-your-own-transport-mechanism)
