---
title: 模型
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';
import modelCustomProviderExample from '../../../../../../examples/docs/models/customProviders.ts?raw';
import setDefaultOpenAIKeyExample from '../../../../../../examples/docs/config/setDefaultOpenAIKey.ts?raw';
import modelSettingsExample from '../../../../../../examples/docs/models/modelSettings.ts?raw';
import promptIdExample from '../../../../../../examples/basic/prompt-id.ts?raw';
import agentWithModelExample from '../../../../../../examples/docs/models/agentWithModel.ts?raw';
import runnerWithModelExample from '../../../../../../examples/docs/models/runnerWithModel.ts?raw';
import gpt5DefaultModelSettingsExample from '../../../../../../examples/docs/models/gpt5DefaultModelSettings.ts?raw';
import setDefaultModelProviderExample from '../../../../../../examples/docs/models/setDefaultModelProvider.ts?raw';
import setTracingExportApiKeyExample from '../../../../../../examples/docs/config/setTracingExportApiKey.ts?raw';

每个智能体最终都会调用一个 LLM。SDK 通过两个轻量接口对模型进行了抽象：

- [`Model`](/openai-agents-js/openai/agents/interfaces/model)——知道如何针对特定 API 发起**一次**请求。
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider)——将人类可读的模型**名称**（例如 `'gpt‑5.2'`）解析为 `Model` 实例。

在日常开发中，你通常只会与模型**名称**交互，偶尔使用 `ModelSettings`。

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="为每个智能体指定模型"
/>

## 模型选择

### 默认模型

当你在初始化 `Agent` 时未指定模型，将使用默认模型。当前默认值是 [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1)，以兼顾兼容性和低延迟。如果你有权限，我们建议将智能体设置为 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) 以获得更高质量，同时显式设置 `modelSettings`。

如果你想切换到其他模型（如 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2)），有两种方式可以配置智能体。

第一种，如果你希望所有未设置自定义模型的智能体都统一使用某个特定模型，请在运行智能体前设置 `OPENAI_DEFAULT_MODEL` 环境变量。

```bash
export OPENAI_DEFAULT_MODEL=gpt-5.2
node my-awesome-agent.js
```

第二种，你可以为 `Runner` 实例设置默认模型。如果智能体未设置模型，则会使用该 `Runner` 的默认模型。

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="为 Runner 设置默认模型"
/>

#### GPT-5.x 模型

当你以这种方式使用任意 GPT-5.x 模型（例如 [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2)）时，SDK 会应用默认 `modelSettings`。它会设置在大多数场景下效果最佳的选项。要调整默认模型的推理强度，请传入你自己的 `modelSettings`：

<Code
  lang="typescript"
  code={gpt5DefaultModelSettingsExample}
  title="自定义 GPT-5 默认设置"
/>

若要降低延迟，推荐在 `gpt-5.2` 上使用 `reasoning.effort: "none"`。gpt-4.1 系列（包括 mini 和 nano 变体）也仍然是构建交互式智能体应用的可靠选择。

#### 非 GPT-5 模型

如果你传入的是非 GPT-5 的模型名称，且未提供自定义 `modelSettings`，SDK 会回退到与任意模型兼容的通用 `modelSettings`。

---

## OpenAI provider 配置

### OpenAI provider

默认的 `ModelProvider` 使用 OpenAI API 解析模型名称。它支持两个不同的端点：

| API              | 用法                                       | 调用 `setOpenAIAPI()`                  |
| ---------------- | ------------------------------------------ | -------------------------------------- |
| Chat Completions | 标准对话与函数调用                         | `setOpenAIAPI('chat_completions')`     |
| Responses        | 新的流式优先生成 API（工具调用、灵活输出） | `setOpenAIAPI('responses')` _（默认）_ |

#### 认证

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="设置默认 OpenAI 密钥"
/>

如果你需要自定义网络设置，也可以通过 `setDefaultOpenAIClient(client)` 接入你自己的 `OpenAI` 客户端。

#### Responses WebSocket 传输

当你将 OpenAI provider 与 Responses API 一起使用时，可以通过 WebSocket 传输发送请求，而不是默认的 HTTP 传输。

你可以通过 `setOpenAIResponsesTransport('websocket')` 全局启用，或在单个 provider 上通过 `new OpenAIProvider({ useResponses: true, useResponsesWebSocket: true })` 启用。

仅为了使用 WebSocket 传输，你不需要 `withResponsesWebSocketSession(...)` 或自定义 `OpenAIProvider`。如果每次运行/请求都重新连接可以接受，那么启用 `setOpenAIResponsesTransport('websocket')` 后，你现有的 `run()` / `Runner.run()` 用法仍可继续工作。

传输选择遵循模型解析逻辑：

- `setOpenAIResponsesTransport('websocket')` 仅影响那些随后通过 OpenAI provider 解析、且使用 Responses API 的字符串模型名称。
- 如果你向 `Agent` 或 `Runner` 传入具体的 `Model` 实例，该实例会按原样使用。`OpenAIResponsesWSModel` 保持 WebSocket，`OpenAIResponsesModel` 保持 HTTP，`OpenAIChatCompletionsModel` 保持 Chat Completions。
- 如果你提供了自己的 `modelProvider`，则由该 provider 控制模型解析。请在其中启用 WebSocket，而不是依赖全局设置器。
- 如果你通过代理、网关或其他 OpenAI 兼容端点进行路由，目标端必须支持 WebSocket `/responses` 端点。你也可能需要显式设置 `websocketBaseURL`。

仅当你希望优化连接复用并更明确地管理 websocket provider 生命周期时，才使用 `withResponsesWebSocketSession(...)` 或自定义 `OpenAIProvider` / `Runner`：

- `withResponsesWebSocketSession(...)`：便捷的作用域生命周期，在回调结束后自动清理。
- 自定义 `OpenAIProvider` / `Runner`：在你自己的应用架构中进行显式生命周期控制（包括关闭时清理）。

尽管名为 `withResponsesWebSocketSession(...)`，它是一个传输生命周期辅助工具，与[会话指南](/openai-agents-js/zh/guides/sessions)中描述的内存 `Session` 接口无关。

如果你使用 websocket 代理或网关，请在 `OpenAIProvider` 上配置 `websocketBaseURL`，或设置 `OPENAI_WEBSOCKET_BASE_URL`。

如果你自行实例化 `OpenAIProvider`，请记住：基于 websocket 的 Responses 模型包装器默认会被缓存以复用连接。请在关闭时调用 `await provider.close()` 以释放这些缓存连接。`withResponsesWebSocketSession(...)` 的主要作用之一就是帮你管理该生命周期：它会创建启用 websocket 的 provider 和 runner，传给你的回调，并在之后始终关闭 provider。临时 provider 使用 `providerOptions`，回调作用域内的 runner 默认项使用 `runnerConfig`。

完整的流式传输 + HITL 示例（使用 Responses WebSocket 传输）见 [`examples/basic/stream-ws.ts`](https://github.com/openai/openai-agents-js/tree/main/examples/basic/stream-ws.ts)。

---

## 模型行为与提示词

### ModelSettings

`ModelSettings` 映射 OpenAI 参数，但与 provider 无关。

| 字段                   | 类型                                                            | 说明                                                                      |
| ---------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------- |
| `temperature`          | `number`                                                        | 创造性与确定性之间的平衡。                                                |
| `topP`                 | `number`                                                        | 核采样。                                                                  |
| `frequencyPenalty`     | `number`                                                        | 惩罚重复 token。                                                          |
| `presencePenalty`      | `number`                                                        | 鼓励生成新 token。                                                        |
| `toolChoice`           | `'auto' \| 'required' \| 'none' \| string`                      | 参见[强制使用工具](/openai-agents-js/zh/guides/agents#forcing-tool-use)。 |
| `parallelToolCalls`    | `boolean`                                                       | 在支持时允许并行函数调用。                                                |
| `truncation`           | `'auto' \| 'disabled'`                                          | token 截断策略。                                                          |
| `maxTokens`            | `number`                                                        | 响应中的最大 token 数。                                                   |
| `store`                | `boolean`                                                       | 持久化响应，用于检索 / RAG 工作流。                                       |
| `promptCacheRetention` | `'in-memory' \| '24h' \| null`                                  | 在支持时控制 provider 的提示词缓存保留策略。                              |
| `reasoning.effort`     | `'none' \| 'minimal' \| 'low' \| 'medium' \| 'high' \| 'xhigh'` | gpt-5.x 模型的推理强度。                                                  |
| `reasoning.summary`    | `'auto' \| 'concise' \| 'detailed'`                             | 控制模型返回多少推理摘要。                                                |
| `text.verbosity`       | `'low' \| 'medium' \| 'high'`                                   | gpt-5.x 等模型的文本详略程度。                                            |
| `providerData`         | `Record<string, any>`                                           | provider 特定的透传选项，会转发到底层模型。                               |

你可以在任一层级附加这些设置：

<Code lang="typescript" code={modelSettingsExample} title="模型设置" />

`Runner` 层级的设置会覆盖冲突的每智能体设置。

---

### 提示词

智能体可以通过 `prompt` 参数进行配置，表示应使用服务器端存储的提示词配置来控制智能体行为。目前此选项仅在你使用 OpenAI [Responses API](https://platform.openai.com/docs/api-reference/responses) 时受支持。

| 字段        | 类型     | 说明                                                                           |
| ----------- | -------- | ------------------------------------------------------------------------------ |
| `promptId`  | `string` | 提示词的唯一标识符。                                                           |
| `version`   | `string` | 你希望使用的提示词版本。                                                       |
| `variables` | `object` | 要替换进提示词的键值对变量。值可以是字符串，或文本、图像、文件等内容输入类型。 |

<Code lang="typescript" code={promptIdExample} title="带提示词的智能体" />

任何额外的智能体配置（如 tools 或 instructions）都会覆盖你在存储提示词中可能配置的值。

---

## 高级 provider 与可观测性

### 自定义模型 provider

实现你自己的 provider 很直接——实现 `ModelProvider` 和 `Model`，然后将 provider 传给 `Runner` 构造函数：

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="最小化自定义 provider"
/>

如果你希望每次 `run()` 调用和每个新构造的 `Runner` 默认都使用同一个 provider，可以在应用启动时设置一次：

<Code
  lang="typescript"
  code={setDefaultModelProviderExample}
  title="设置默认模型 provider"
/>

当你的应用统一使用非 OpenAI provider，且你不想到处传递自定义 `Runner` 时，这很有用。

如果你希望使用现成的非 OpenAI 模型适配器，请参见[使用 AI SDK 指定任意模型](/openai-agents-js/zh/extensions/ai-sdk)。

---

### 追踪凭据

在受支持的服务器运行时中，追踪默认已启用。仅当你希望追踪导出使用与默认 OpenAI API 密钥不同的凭据时，才使用 `setTracingExportApiKey()`：

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="设置追踪导出 API 密钥"
/>

这会使用该凭据将追踪发送到 [OpenAI dashboard](https://platform.openai.com/traces)。有关导出器自定义（如自定义 ingest 端点或重试调优），请参见[追踪指南](/openai-agents-js/zh/guides/tracing#openai-tracing-exporter)。

---

## 后续步骤

- 探索[运行智能体](/openai-agents-js/zh/guides/running-agents)。
- 通过[工具](/openai-agents-js/zh/guides/tools)为你的模型赋予超能力。
- 按需添加[护栏](/openai-agents-js/zh/guides/guardrails)或[追踪](/openai-agents-js/zh/guides/tracing)。
