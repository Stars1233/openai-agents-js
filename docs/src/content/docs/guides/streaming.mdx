---
title: Streaming
description: Stream agent output in real time using the Runner
---

import { Code } from '@astrojs/starlight/components';
import basicStreamingExample from '../../../../../examples/docs/streaming/basicStreaming.ts?raw';
import nodeTextStreamExample from '../../../../../examples/docs/streaming/nodeTextStream.ts?raw';
import handleAllEventsExample from '../../../../../examples/docs/streaming/handleAllEvents.ts?raw';
import streamedHITLExample from '../../../../../examples/docs/streaming/streamedHITL.ts?raw';
import runRawModelStreamEventExample from '../../../../../examples/docs/streaming/runRawModelStreamEvent.ts?raw';
import runItemStreamEventExample from '../../../../../examples/docs/streaming/runItemStreamEvent.ts?raw';
import runAgentUpdatedStreamEventExample from '../../../../../examples/docs/streaming/runAgentUpdatedStreamEvent.ts?raw';

The Agents SDK can deliver output from the model and other execution
steps incrementally. Streaming keeps your UI responsive and avoids
waiting for the entire final result before updating the user.

## Enabling streaming

Pass a `{ stream: true }` option to `Runner.run()` to obtain a streaming
object rather than a full result:

<Code
  lang="typescript"
  code={basicStreamingExample}
  title="Enabling streaming"
/>

When streaming is enabled the returned `stream` implements the
`AsyncIterable` interface. Each yielded event is an object describing
what happened within the run. The stream yields one of three event types, each describing a different part of the agent's execution.
Most applications only want the model's
text though, so the stream provides helpers.

### Get the text output

Call `stream.toTextStream()` to obtain a stream of the emitted text.
When `compatibleWithNodeStreams` is `true` the return value is a regular
Node.js `Readable`. We can pipe it directly into `process.stdout` or
another destination.

<Code
  lang="typescript"
  code={nodeTextStreamExample}
  title="Logging out the text as it arrives"
  meta={`{13-17}`}
/>

The promise `stream.completed` resolves once the run and all pending
callbacks are completed. Always await it if you want to ensure there is
no more output. This includes post-processing work such as session persistence
or history compaction hooks that finish after the last text token arrives.

`toTextStream()` only emits assistant text. Tool calls, handoffs, approvals,
and other runtime events are available from the full event stream.

### Listen to all events

You can use a `for await` loop to inspect each event as it arrives.
Useful information includes low level model events, any agent switches
and SDK specific run information:

<Code
  lang="typescript"
  code={handleAllEventsExample}
  title="Listening to all events"
/>

See [the streamed example](https://github.com/openai/openai-agents-js/tree/main/examples/agent-patterns/streamed.ts)
for a fully worked script that prints both the plain text stream and the
raw event stream.

### Responses WebSocket transport (optional)

The streaming APIs on this page also work with the OpenAI Responses WebSocket transport.

Enable it globally with `setOpenAIResponsesTransport('websocket')`, or use your own `OpenAIProvider` with `useResponsesWebSocket: true`.

You do not need `withResponsesWebSocketSession(...)` or a custom `OpenAIProvider` just to stream over WebSocket. If reconnecting between runs is acceptable, `run()` / `Runner.run(..., { stream: true })` still works after enabling the transport.

Use `withResponsesWebSocketSession(...)` or a custom `OpenAIProvider` / `Runner` when you want connection reuse and more explicit provider lifecycle control.

Continuation with `previousResponseId` uses the same semantics as the HTTP transport. The difference
is just the transport and connection lifecycle.

If you build the provider yourself, remember to call `await provider.close()` when shutting down.
Websocket-backed model wrappers are cached for reuse by default, and closing the provider releases
those connections. `withResponsesWebSocketSession(...)` gives you the same reuse but scopes cleanup
to a single callback automatically.

See [`examples/basic/stream-ws.ts`](https://github.com/openai/openai-agents-js/tree/main/examples/basic/stream-ws.ts) for a complete example with streaming, tool calls, approvals, and `previousResponseId`.

## Event types

The stream yields three different event types:

### raw_model_stream_event

<Code
  lang="typescript"
  code={runRawModelStreamEventExample}
  title="RunRawModelStreamEvent"
/>

Example:

```json
{
  "type": "raw_model_stream_event",
  "data": {
    "type": "output_text_delta",
    "delta": "Hello"
  }
}
```

### run_item_stream_event

<Code
  lang="typescript"
  code={runItemStreamEventExample}
  title="RunItemStreamEvent"
/>

`name` identifies which kind of item was produced:

| `name`                    | Meaning                                           |
| ------------------------- | ------------------------------------------------- |
| `message_output_created`  | A message output item was created.                |
| `handoff_requested`       | The model requested a handoff.                    |
| `handoff_occurred`        | The runtime completed a handoff to another agent. |
| `tool_called`             | A tool call item was emitted.                     |
| `tool_output`             | A tool result item was emitted.                   |
| `reasoning_item_created`  | A reasoning item was emitted.                     |
| `tool_approval_requested` | A tool call paused for human approval.            |

Example handoff payload:

```json
{
  "type": "run_item_stream_event",
  "name": "handoff_occurred",
  "item": {
    "type": "handoff_call",
    "id": "h1",
    "status": "completed",
    "name": "transfer_to_refund_agent"
  }
}
```

### agent_updated_stream_event

<Code
  lang="typescript"
  code={runAgentUpdatedStreamEventExample}
  title="RunAgentUpdatedStreamEvent"
/>

Example:

```json
{
  "type": "agent_updated_stream_event",
  "agent": {
    "name": "Refund Agent"
  }
}
```

## Human in the loop while streaming

Streaming is compatible with handoffs that pause execution (for example
when a tool requires approval). The `interruptions` field on the stream
object exposes the pending approvals, and you can continue execution by
calling `state.approve()` or `state.reject()` for each of them.
After the stream pauses, `stream.completed` resolves and `stream.interruptions`
contains the approvals to handle. Executing again with `{ stream: true }`
resumes streaming output.

<Code
  lang="typescript"
  code={streamedHITLExample}
  title="Handling human approval while streaming"
/>

A fuller example that interacts with the user is
[`human-in-the-loop-stream.ts`](https://github.com/openai/openai-agents-js/tree/main/examples/agent-patterns/human-in-the-loop-stream.ts).

## Tips

- Remember to wait for `stream.completed` before exiting to ensure all
  output has been flushed.
- The initial `{ stream: true }` option only applies to the call where it
  is provided. If you re-run with a `RunState` you must specify the
  option again.
- If your application only cares about the textual result prefer
  `toTextStream()` to avoid dealing with individual event objects.

With streaming and the event system you can integrate an agent into a
chat interface, terminal application or any place where users benefit
from incremental updates.
