---
title: Models
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';
import modelCustomProviderExample from '../../../../../examples/docs/models/customProviders.ts?raw';
import setDefaultOpenAIKeyExample from '../../../../../examples/docs/config/setDefaultOpenAIKey.ts?raw';
import modelSettingsExample from '../../../../../examples/docs/models/modelSettings.ts?raw';
import promptIdExample from '../../../../../examples/basic/prompt-id.ts?raw';
import agentWithModelExample from '../../../../../examples/docs/models/agentWithModel.ts?raw';
import runnerWithModelExample from '../../../../../examples/docs/models/runnerWithModel.ts?raw';
import gpt5DefaultModelSettingsExample from '../../../../../examples/docs/models/gpt5DefaultModelSettings.ts?raw';
import setDefaultModelProviderExample from '../../../../../examples/docs/models/setDefaultModelProvider.ts?raw';
import setTracingExportApiKeyExample from '../../../../../examples/docs/config/setTracingExportApiKey.ts?raw';

Every Agent ultimately calls an LLM. The SDK abstracts models behind two lightweight interfaces:

- [`Model`](/openai-agents-js/openai/agents/interfaces/model) – knows how to make _one_ request against a specific API.
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider) – resolves human‑readable model **names** (e.g. `'gpt‑5.2'`) to `Model` instances.

In day‑to‑day work you normally only interact with model **names** and occasionally `ModelSettings`.

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="Specifying a model per‑agent"
/>

## Choosing models

### Default model

When you don't specify a model when initializing an `Agent`, the default model will be used. The default is currently [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) for compatibility and low latency. If you have access, we recommend setting your agents to [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) for higher quality while keeping explicit `modelSettings`.

If you want to switch to other models like [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2), there are two ways to configure your agents.

First, if you want to consistently use a specific model for all agents that do not set a custom model, set the `OPENAI_DEFAULT_MODEL` environment variable before running your agents.

```bash
export OPENAI_DEFAULT_MODEL=gpt-5.2
node my-awesome-agent.js
```

Second, you can set a default model for a `Runner` instance. If you don't set a model for an agent, this `Runner`'s default model will be used.

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="Set a default model for a Runner"
/>

#### GPT-5.x models

When you use any GPT-5.x model such as [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) in this way, the SDK applies default `modelSettings`. It sets the ones that work the best for most use cases. To adjust the reasoning effort for the default model, pass your own `modelSettings`:

<Code
  lang="typescript"
  code={gpt5DefaultModelSettingsExample}
  title="Customize GPT-5 default settings"
/>

For lower latency, using `reasoning.effort: "none"` with `gpt-5.2` is recommended. The gpt-4.1 family (including mini and nano variants) also remains a solid choice for building interactive agent apps.

#### Non-GPT-5 models

If you pass a non–GPT-5 model name without custom `modelSettings`, the SDK reverts to generic `modelSettings` compatible with any model.

---

## OpenAI provider configuration

### The OpenAI provider

The default `ModelProvider` resolves names using the OpenAI APIs. It supports two distinct
endpoints:

| API              | Usage                                                             | Call `setOpenAIAPI()`                   |
| ---------------- | ----------------------------------------------------------------- | --------------------------------------- |
| Chat Completions | Standard chat & function calls                                    | `setOpenAIAPI('chat_completions')`      |
| Responses        | New streaming‑first generative API (tool calls, flexible outputs) | `setOpenAIAPI('responses')` _(default)_ |

#### Authentication

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="Set default OpenAI key"
/>

You can also plug your own `OpenAI` client via `setDefaultOpenAIClient(client)` if you need
custom networking settings.

#### Responses WebSocket transport

When you use the OpenAI provider with the Responses API, you can send requests over a WebSocket transport instead of the default HTTP transport.

Enable it globally with `setOpenAIResponsesTransport('websocket')`, or enable it per provider with `new OpenAIProvider({ useResponses: true, useResponsesWebSocket: true })`.

You do not need `withResponsesWebSocketSession(...)` or a custom `OpenAIProvider` just to use the WebSocket transport. If reconnecting for each run/request is acceptable, your existing `run()` / `Runner.run()` usage will continue to work after enabling `setOpenAIResponsesTransport('websocket')`.

Transport selection follows model resolution:

- `setOpenAIResponsesTransport('websocket')` only affects string model names that are later resolved through the OpenAI provider while using the Responses API.
- If you pass a concrete `Model` instance to an `Agent` or `Runner`, that instance is used as-is. `OpenAIResponsesWSModel` stays on WebSocket, `OpenAIResponsesModel` stays on HTTP, and `OpenAIChatCompletionsModel` stays on Chat Completions.
- If you provide your own `modelProvider`, that provider controls model resolution. Enable WebSocket there instead of relying on the global setter.
- If you route through a proxy, gateway, or other OpenAI-compatible endpoint, the target must support the WebSocket `/responses` endpoint. You may also need to set `websocketBaseURL` explicitly.

Use `withResponsesWebSocketSession(...)` or a custom `OpenAIProvider` / `Runner` only when you want to optimize connection reuse and manage the websocket provider lifecycle more explicitly:

- `withResponsesWebSocketSession(...)`: convenient scoped lifecycle with automatic cleanup after the callback.
- Custom `OpenAIProvider` / `Runner`: explicit lifecycle control (including shutdown cleanup) in your own app architecture.

Despite the name, `withResponsesWebSocketSession(...)` is a transport lifecycle helper and is unrelated to the memory `Session` interface described in the [sessions guide](/openai-agents-js/guides/sessions).

If you use a websocket proxy or gateway, configure `websocketBaseURL` on `OpenAIProvider` or set `OPENAI_WEBSOCKET_BASE_URL`.

If you instantiate `OpenAIProvider` yourself, remember that websocket-backed Responses model wrappers are cached by default for connection reuse. Call `await provider.close()` during shutdown to release those cached connections. `withResponsesWebSocketSession(...)` exists largely to manage that lifecycle for you: it creates a websocket-enabled provider and runner, passes them to your callback, and always closes the provider afterward. Use `providerOptions` for the temporary provider and `runnerConfig` for callback-scoped runner defaults.

See [`examples/basic/stream-ws.ts`](https://github.com/openai/openai-agents-js/tree/main/examples/basic/stream-ws.ts) for a full streaming + HITL example using the Responses WebSocket transport.

---

## Model behavior and prompts

### ModelSettings

`ModelSettings` mirrors the OpenAI parameters but is provider‑agnostic.

| Field                  | Type                                                            | Notes                                                                     |
| ---------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------- |
| `temperature`          | `number`                                                        | Creativity vs. determinism.                                               |
| `topP`                 | `number`                                                        | Nucleus sampling.                                                         |
| `frequencyPenalty`     | `number`                                                        | Penalise repeated tokens.                                                 |
| `presencePenalty`      | `number`                                                        | Encourage new tokens.                                                     |
| `toolChoice`           | `'auto' \| 'required' \| 'none' \| string`                      | See [forcing tool use](/openai-agents-js/guides/agents#forcing-tool-use). |
| `parallelToolCalls`    | `boolean`                                                       | Allow parallel function calls where supported.                            |
| `truncation`           | `'auto' \| 'disabled'`                                          | Token truncation strategy.                                                |
| `maxTokens`            | `number`                                                        | Maximum tokens in the response.                                           |
| `store`                | `boolean`                                                       | Persist the response for retrieval / RAG workflows.                       |
| `promptCacheRetention` | `'in-memory' \| '24h' \| null`                                  | Controls provider prompt-cache retention when supported.                  |
| `reasoning.effort`     | `'none' \| 'minimal' \| 'low' \| 'medium' \| 'high' \| 'xhigh'` | Reasoning effort for gpt-5.x models.                                      |
| `reasoning.summary`    | `'auto' \| 'concise' \| 'detailed'`                             | Controls how much reasoning summary the model returns.                    |
| `text.verbosity`       | `'low' \| 'medium' \| 'high'`                                   | Text verbosity for gpt-5.x etc.                                           |
| `providerData`         | `Record<string, any>`                                           | Provider-specific passthrough options forwarded to the underlying model.  |

Attach settings at either level:

<Code lang="typescript" code={modelSettingsExample} title="Model settings" />

`Runner`‑level settings override any conflicting per‑agent settings.

---

### Prompt

Agents can be configured with a `prompt` parameter, indicating a server-stored
prompt configuration that should be used to control the Agent's behavior. Currently,
this option is only supported when you use the OpenAI
[Responses API](https://platform.openai.com/docs/api-reference/responses).

| Field       | Type     | Notes                                                                                                                                  |
| ----------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| `promptId`  | `string` | Unique identifier for a prompt.                                                                                                        |
| `version`   | `string` | Version of the prompt you wish to use.                                                                                                 |
| `variables` | `object` | A key/value pair of variables to substitute into the prompt. Values can be strings or content input types like text, images, or files. |

<Code lang="typescript" code={promptIdExample} title="Agent with prompt" />

Any additional agent configuration, like tools or instructions, will override the
values you may have configured in your stored prompt.

---

## Advanced providers and observability

### Custom model providers

Implementing your own provider is straightforward – implement `ModelProvider` and `Model` and
pass the provider to the `Runner` constructor:

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="Minimal custom provider"
/>

If you want every `run()` call and every newly constructed `Runner` to use the same provider by
default, set it once during app startup:

<Code
  lang="typescript"
  code={setDefaultModelProviderExample}
  title="Set a default model provider"
/>

This is useful when your app standardizes on a non-OpenAI provider and you do not want to pass a
custom `Runner` everywhere.

If you want a ready-made adapter for non-OpenAI models, see [Using any model with Vercel's AI SDK](/openai-agents-js/extensions/ai-sdk).

---

### Tracing credentials

Tracing is already enabled by default in supported server runtimes. Use
`setTracingExportApiKey()` only when trace export should use a different credential than the
default OpenAI API key:

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="Set tracing export API key"
/>

This sends traces to the [OpenAI dashboard](https://platform.openai.com/traces) using that
credential. For exporter customization such as custom ingest endpoints or retry tuning, see the
[Tracing guide](/openai-agents-js/guides/tracing#openai-tracing-exporter).

---

## Next steps

- Explore [running agents](/openai-agents-js/guides/running-agents).
- Give your models super‑powers with [tools](/openai-agents-js/guides/tools).
- Add [guardrails](/openai-agents-js/guides/guardrails) or [tracing](/openai-agents-js/guides/tracing) as needed.
