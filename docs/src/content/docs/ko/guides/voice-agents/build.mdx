---
title: 음성 에이전트 구축
description: Learn how to build voice agents using the OpenAI Agents SDK, what features are available, how to architecture your application, and more.
---

import { Steps, Aside, Code } from '@astrojs/starlight/components';
import createAgentExample from '../../../../../../../examples/docs/voice-agents/createAgent.ts?raw';
import multiAgentsExample from '../../../../../../../examples/docs/voice-agents/multiAgents.ts?raw';
import createSessionExample from '../../../../../../../examples/docs/voice-agents/createSession.ts?raw';
import configureSessionExample from '../../../../../../../examples/docs/voice-agents/configureSession.ts?raw';
import handleAudioExample from '../../../../../../../examples/docs/voice-agents/handleAudio.ts?raw';
import defineToolExample from '../../../../../../../examples/docs/voice-agents/defineTool.ts?raw';
import toolApprovalEventExample from '../../../../../../../examples/docs/voice-agents/toolApprovalEvent.ts?raw';
import guardrailsExample from '../../../../../../../examples/docs/voice-agents/guardrails.ts?raw';
import guardrailSettingsExample from '../../../../../../../examples/docs/voice-agents/guardrailSettings.ts?raw';
import audioInterruptedExample from '../../../../../../../examples/docs/voice-agents/audioInterrupted.ts?raw';
import sessionInterruptExample from '../../../../../../../examples/docs/voice-agents/sessionInterrupt.ts?raw';
import sessionHistoryExample from '../../../../../../../examples/docs/voice-agents/sessionHistory.ts?raw';
import historyUpdatedExample from '../../../../../../../examples/docs/voice-agents/historyUpdated.ts?raw';
import updateHistoryExample from '../../../../../../../examples/docs/voice-agents/updateHistory.ts?raw';
import customWebRTCTransportExample from '../../../../../../../examples/docs/voice-agents/customWebRTCTransport.ts?raw';
import websocketSessionExample from '../../../../../../../examples/docs/voice-agents/websocketSession.ts?raw';
import transportEventsExample from '../../../../../../../examples/docs/voice-agents/transportEvents.ts?raw';
import thinClientExample from '../../../../../../../examples/docs/voice-agents/thinClient.ts?raw';
import toolHistoryExample from '../../../../../../../examples/docs/voice-agents/toolHistory.ts?raw';
import sendMessageExample from '../../../../../../../examples/docs/voice-agents/sendMessage.ts?raw';
import serverAgentExample from '../../../../../../../examples/docs/voice-agents/serverAgent.ts?raw';
import delegationAgentExample from '../../../../../../../examples/docs/voice-agents/delegationAgent.ts?raw';
import turnDetectionExample from '../../../../../../../examples/docs/voice-agents/turnDetection.ts?raw';

<Aside type="caution">

아키텍처는 초기에 선택하세요:

- `OpenAIRealtimeWebRTC` 는 브라우저에서 가장 간단한 경로이며 오디오 입력/출력을 대신 처리합니다
- `OpenAIRealtimeWebSocket` 은 더 많은 제어권을 제공하지만, 오디오 캡처와 재생을 직접 관리해야 합니다
- 함수 도구는 `RealtimeSession` 이 실행되는 위치에서 실행됩니다. 세션이 브라우저에서 실행되면 도구도 브라우저에서 실행됩니다
- Realtime 핸드오프는 동일한 음성과 모델을 유지합니다. 다른 백엔드 모델이 필요하면 핸드오프 대신 도구를 통한 위임을 사용하세요

</Aside>

## 세션 설정

### 오디오 처리

기본 `OpenAIRealtimeWebRTC` 와 같은 일부 전송 레이어는
오디오 입력과 출력을 자동으로 처리합니다. `OpenAIRealtimeWebSocket` 같은 다른 전송 방식에서는
세션 오디오를 직접 처리해야 합니다:

<Code lang="typescript" code={handleAudioExample} />

기반 전송이 이를 지원하면 `session.muted` 는 현재 음소거 상태를 반환하고
`session.mute(true | false)` 는 마이크 캡처를 전환합니다. `OpenAIRealtimeWebSocket` 은
음소거를 구현하지 않으므로 `session.muted` 는 `null` 을 반환하고 `session.mute()` 는 오류를 발생시킵니다. 따라서 websocket
설정에서는 마이크를 다시 활성화해야 할 때까지 캡처를 중지하고 `sendAudio()` 호출을 멈춰야 합니다.

### 세션 구성

세션은 생성 시 [`RealtimeSession`](/openai-agents-js/openai/agents-realtime/classes/realtimesession/) 에 추가 옵션을 전달하거나
`connect(...)` 호출 시 옵션을 전달해 구성할 수 있습니다.

<Code lang="typescript" code={configureSessionExample} />

이 전송 레이어에서는 [session](https://platform.openai.com/docs/api-reference/realtime-client-events/session/update) 에 맞는 모든 매개변수를 전달할 수 있습니다.

`outputModalities`, `audio.input`, `audio.output` 를 사용하는 최신 구성 형태를 권장합니다. 레거시
최상위 필드인 `modalities`, `inputAudioFormat`, `outputAudioFormat`,
`inputAudioTranscription`, `turnDetection` 도 하위 호환성을 위해 계속 정규화되지만,
새 코드는 여기 표시된 중첩 `audio` 구조를 사용해야 합니다.

새로 추가된 매개변수 중 [RealtimeSessionConfig](/openai-agents-js/openai/agents-realtime/type-aliases/realtimesessionconfig/) 에 대응 항목이 없다면 `providerData` 를 사용할 수 있습니다. `providerData` 에 전달한 모든 값은 `session` 객체의 일부로 직접 전달됩니다.

생성 시 설정할 수 있는 추가 `RealtimeSession` 옵션:

| 옵션                                          | 타입                              | 목적                                                                                     |
| --------------------------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------- |
| `context`                                     | `TContext`                        | 세션 컨텍스트에 병합되는 추가 로컬 컨텍스트                                              |
| `historyStoreAudio`                           | `boolean`                         | 로컬 히스토리 스냅샷에 오디오 데이터 저장(기본 비활성화)                                 |
| `outputGuardrails`                            | `RealtimeOutputGuardrail[]`       | 세션의 출력 가드레일([Guardrails](#guardrails) 참조)                                     |
| `outputGuardrailSettings`                     | `{ debounceTextLength?: number }` | 가드레일 실행 간격. 기본값은 `100`, 전체 텍스트가 준비된 뒤 한 번만 실행하려면 `-1` 사용 |
| `tracingDisabled`                             | `boolean`                         | 세션의 트레이싱 비활성화                                                                 |
| `groupId`                                     | `string`                          | 세션 또는 백엔드 실행 간 트레이스를 그룹화                                               |
| `traceMetadata`                               | `Record<string, any>`             | 세션 트레이스에 첨부할 사용자 지정 메타데이터                                            |
| `workflowName`                                | `string`                          | 트레이스 워크플로의 읽기 쉬운 이름                                                       |
| `automaticallyTriggerResponseForMcpToolCalls` | `boolean`                         | MCP 도구 호출 완료 시 모델 응답 자동 트리거(기본값: `true`)                              |
| `toolErrorFormatter`                          | `ToolErrorFormatter`              | 모델에 반환되는 도구 승인 거부 메시지 사용자 지정                                        |

`connect(...)` 옵션:

| 옵션     | 타입                                          | 목적                                       |
| -------- | --------------------------------------------- | ------------------------------------------ |
| `apiKey` | `string \| (() => string \| Promise<string>)` | 이 연결에 사용할 API 키(또는 지연 로더)    |
| `model`  | `OpenAIRealtimeModels \| string`              | 전송 연결에 사용할 선택적 모델 재정의      |
| `url`    | `string`                                      | 선택적 사용자 지정 Realtime 엔드포인트 URL |
| `callId` | `string`                                      | 기존 SIP 시작 통화/세션에 연결             |

## 에이전트 기능

### 핸드오프

일반 에이전트와 마찬가지로 핸드오프를 사용해 에이전트를 여러 에이전트로 분리하고 이들 사이를 오케스트레이션함으로써 에이전트 성능을 높이고 문제 범위를 더 잘 한정할 수 있습니다.

<Code lang="typescript" code={multiAgentsExample} />

일반 에이전트와 달리, 실시간 에이전트에서 핸드오프는 약간 다르게 동작합니다. 핸드오프가 수행되면 진행 중인 세션이 새 에이전트 구성으로 업데이트됩니다. 이 때문에 에이전트는 진행 중인 대화 히스토리에 자동으로 접근할 수 있고, 현재 입력 필터는 적용되지 않습니다.

또한 이는 핸드오프 과정에서 `voice` 나 `model` 을 변경할 수 없음을 의미합니다. 연결 대상도 다른 실시간 에이전트로만 제한됩니다. 예를 들어 `gpt-5-mini` 같은 추론 모델처럼 다른 모델이 필요하다면 [delegation through tools](#delegation-through-tools) 를 사용할 수 있습니다.

### 도구

일반 에이전트처럼 실시간 에이전트도 작업 수행을 위해 도구를 호출할 수 있습니다. Realtime 은 **함수 도구**(로컬 실행)와 **호스티드 MCP 서버 도구**(Realtime API 가 원격 실행)를 지원합니다. 함수 도구는 일반 에이전트에서 쓰는 것과 같은 `tool()` 헬퍼로 정의할 수 있습니다.

<Code lang="typescript" code={defineToolExample} />

#### 함수 도구

함수 도구는 `RealtimeSession` 과 같은 환경에서 실행됩니다. 즉 세션을 브라우저에서 실행하면 도구도 브라우저에서 실행됩니다. 민감한 작업이 필요하면 도구 내부에서 백엔드를 호출하고 권한이 필요한 작업은 서버에서 수행하도록 하세요.

이 방식으로 브라우저 측 도구를 서버 측 로직으로 연결되는 얇은 백채널로 사용할 수 있습니다. 예를 들어 [`examples/realtime-next`](https://github.com/openai/openai-agents-js/tree/main/examples/realtime-next) 는 브라우저에 `refundBackchannel` 도구를 정의해 요청과 현재 대화 히스토리를 서버의 `handleRefundRequest(...)` 로 전달하며, 서버에서는 별도의 `Runner` 가 다른 에이전트 또는 모델을 사용해 환불을 평가한 뒤 결과를 음성 세션으로 반환할 수 있습니다.

#### 호스티드 MCP 서버 도구

호스티드 MCP 서버 도구는 `hostedMcpTool` 로 구성할 수 있으며 원격에서 실행됩니다. MCP 도구 가용성이 바뀌면 세션은 `mcp_tools_changed` 를 emit 합니다. MCP 도구 호출 완료 후 세션이 모델 응답을 자동 트리거하지 않게 하려면 `automaticallyTriggerResponseForMcpToolCalls: false` 로 설정하세요.

현재 필터링된 MCP 도구 목록은 `session.availableMcpTools` 로도 확인할 수 있습니다. 이
속성과 `mcp_tools_changed` 이벤트는 활성 에이전트에서 활성화된 호스티드 MCP 서버만 반영하며,
에이전트 구성의 `allowed_tools` 필터 적용 이후 결과를 보여줍니다.

#### 백그라운드 결과

도구 실행 중에는 에이전트가 사용자의 새 요청을 처리할 수 없습니다. 사용자 경험을 개선하는 한 가지 방법은 도구를 실행하기 직전에 안내하도록 하거나, 도구 실행 시간을 벌기 위한 특정 문구를 말하도록 에이전트에 지시하는 것입니다.

함수 도구가 즉시 다음 모델 응답을 트리거하지 않고 종료되어야 한다면 `@openai/agents/realtime` 의 `backgroundResult(output)` 를 반환하세요.
이렇게 하면 도구 출력이 세션으로 전송되면서 응답 트리거는 사용자가 제어할 수 있습니다.

#### 타임아웃

함수 도구 타임아웃 옵션(`timeoutMs`, `timeoutBehavior`, `timeoutErrorFunction`)은 Realtime 세션에서도 동일하게 동작합니다. 기본값 `error_as_result` 에서는 타임아웃 메시지가 도구 출력으로 전송됩니다. `raise_exception` 에서는 세션이 [`ToolTimeoutError`](/openai-agents-js/openai/agents-core/classes/tooltimeouterror) 와 함께 `error` 이벤트를 emit 하며 해당 호출의 도구 출력은 전송하지 않습니다.

#### 대화 히스토리 접근

에이전트가 특정 도구를 호출할 때 사용한 인수 외에도, Realtime Session 이 추적하는 현재 대화 히스토리의 스냅샷에 접근할 수 있습니다. 이는 현재 대화 상태를 기반으로 더 복잡한 작업을 수행해야 하거나 [tools for delegation](#delegation-through-tools) 을 사용할 계획일 때 유용합니다.

<Code lang="typescript" code={toolHistoryExample} />

<Aside type="note">
  전달되는 히스토리는 도구 호출 시점의 스냅샷입니다. 사용자가 마지막으로 말한
  내용의 전사는 아직 준비되지 않았을 수 있습니다.
</Aside>

#### 도구 실행 전 승인

도구를 `needsApproval: true` 로 정의하면 에이전트는 도구 실행 전에 `tool_approval_requested` 이벤트를 emit 합니다.

이 이벤트를 수신하면 사용자에게 도구 호출을 승인하거나 거절하는 UI 를 표시할 수 있습니다.

<Code lang="typescript" code={toolApprovalEventExample} />

<Aside type="note">
  음성 에이전트가 도구 호출 승인을 기다리는 동안에는 사용자의 새 요청을 처리할
  수 없습니다.
</Aside>

### 가드레일

가드레일은 에이전트가 말한 내용이 규칙 집합을 위반했는지 모니터링하고 응답을 즉시 중단하는 방법을 제공합니다. 이 가드레일 검사는 에이전트 응답의 전사를 기반으로 수행되므로 모델의 텍스트 출력이 활성화되어 있어야 합니다(기본 활성화).

제공한 가드레일은 모델 응답이 반환되는 동안 비동기로 실행되므로, 예를 들어 "특정 금지어 언급" 같은 사전 정의된 분류 트리거에 따라 응답을 중단할 수 있습니다.

가드레일이 발동되면 세션은 `guardrail_tripped` 이벤트를 emit 합니다. 이벤트는 가드레일을 트리거한 `itemId` 를 담은 `details` 객체도 제공합니다.

<Code lang="typescript" code={guardrailsExample} />

기본적으로 가드레일은 100 자마다 실행되고 최종 전사가 준비되면 다시 실행됩니다. 일반적으로 텍스트를 말하는 시간이 전사를 생성하는 시간보다 길기 때문에, 사용자에게 들리기 전에 위험한 출력을 차단할 수 있는 경우가 많습니다.

이 동작을 변경하려면 세션에 `outputGuardrailSettings` 객체를 전달하세요.

응답 끝에서 완전히 생성된 전사를 한 번만 평가하려면 `debounceTextLength: -1` 로 설정하세요.

<Code lang="typescript" code={guardrailSettingsExample} />

## 상호작용 흐름

### 턴 감지 / 음성 활동 감지

Realtime Session 은 Realtime API 에 내장된 [voice activity detection modes of the Realtime API](https://platform.openai.com/docs/guides/realtime-vad) 를 사용해 사용자의 발화를 자동 감지하고 새 턴을 트리거합니다.

음성 활동 감지 모드는
세션 구성에서 `audio.input.turnDetection` 을 전달해 변경할 수 있습니다.

<Code lang="typescript" code={turnDetectionExample} />

턴 감지 설정을 조정하면 원치 않는 인터럽션(중단 처리) 과 침묵 상황을 보정하는 데 도움이 됩니다. 각 설정의 자세한 내용은 [Realtime API documentation for more details on the different settings](https://platform.openai.com/docs/guides/realtime-vad) 를 확인하세요

### 인터럽션(중단 처리)

내장 음성 활동 감지를 사용할 때 에이전트의 말 위에 사용자가 말하면 자동으로
에이전트가 이를 감지해 컨텍스트를 업데이트합니다. 또한
`audio_interrupted` 이벤트를 emit 합니다. 이를 사용해 모든 오디오 재생을 즉시 중지할 수 있습니다(WebSocket 연결에만 해당).

<Code lang="typescript" code={audioInterruptedExample} />

수동 인터럽션(중단 처리) 이 필요하다면, 예를 들어 UI 에 "중지" 버튼을 제공하려는 경우
`interrupt()` 를 직접 호출할 수 있습니다:

<Code lang="typescript" code={sessionInterruptExample} />

어느 방식이든 Realtime Session 은 에이전트의 생성 중단, 사용자에게 전달된 내용에 대한 인식 절단, 히스토리 업데이트를 모두 처리합니다.

WebRTC 로 에이전트에 연결한 경우 오디오 출력도 지웁니다. WebSocket 을 사용하는 경우에는 재생 대기열에 쌓인 오디오 재생을 중지하도록 직접 처리해야 합니다.

### 텍스트 입력

에이전트에 텍스트 입력을 보내려면 `RealtimeSession` 의 `sendMessage` 메서드를 사용할 수 있습니다.

이는 사용자가 에이전트와 두 가지 모달리티 모두로 상호작용하게 하거나,
대화에 추가 컨텍스트를 제공하려는 경우 유용합니다.

<Code lang="typescript" code={sendMessageExample} />

## 대화 상태와 위임

### 대화 히스토리 관리

`RealtimeSession` 은 `history` 속성에서 대화 히스토리를 자동으로 관리합니다:

이를 사용해 히스토리를 고객에게 렌더링하거나 추가 작업을 수행할 수 있습니다. 이
히스토리는 대화 중 계속 변경되므로 `history_updated` 이벤트를 수신하면 됩니다.

메시지를 완전히 제거하거나 전사를 업데이트하는 등 히스토리를 수정하려면
`updateHistory` 메서드를 사용할 수 있습니다.

<Code lang="typescript" code={updateHistoryExample} />

#### 제한 사항

1. 현재는 함수 도구 호출을 사후에 업데이트/변경할 수 없습니다
2. 히스토리의 텍스트 출력은 전사와 텍스트 모달리티가 활성화되어 있어야 합니다
3. 인터럽션(중단 처리) 으로 잘린 응답에는 전사가 없습니다

### 도구를 통한 위임

![Delegation through tools](https://cdn.openai.com/API/docs/diagram-speech-to-speech-agent-tools.png)

대화 히스토리와 도구 호출을 결합하면, 더 복잡한 작업을 수행하도록 대화를 다른 백엔드 에이전트에 위임한 뒤 그 결과를 사용자에게 다시 전달할 수 있습니다.

<Code lang="typescript" code={delegationAgentExample} />

아래 코드는 서버에서 실행됩니다. 이 예시에서는 Next.js 의 서버 액션을 통해 실행됩니다.

<Code lang="typescript" code={serverAgentExample} />
